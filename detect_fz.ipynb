{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04a8ddc",
   "metadata": {},
   "source": [
    "Я для себя искал вхождения через re.findall циклом по словам из блэклиста, но решил, что в проде нужно чтобы работало быстрее, поэтому добавил в блэклисты побольше форм слов (прилагательные, глаголы) и поместил их в множества, где поиск О(1).\n",
    "Может вообще зря парился и основная времязатрата здесь pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4ab5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "#!pip install pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "### Подгружаем стоп-листы из текстовых файлов\n",
    "def get_stop_lists():\n",
    "    drug_words = []\n",
    "    with open ('drugs.txt', 'r', encoding = 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            drug_words.append(line[:-1])\n",
    "    suicide_words = []\n",
    "    with open ('suicide.txt', 'r', encoding = 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            suicide_words.append(line[:-1])\n",
    "    suicide_bigrams = []\n",
    "    with open ('suicide_bigrams.txt', 'r', encoding = 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            suicide_bigrams.append(line[:-1])\n",
    "    lgbt_words = []\n",
    "    with open ('lgbt.txt', 'r', encoding = 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            lgbt_words.append(line[:-1])\n",
    "    return (set(drug_words), set(suicide_words), set(suicide_bigrams), set(lgbt_words))\n",
    "\n",
    "### Функция, проверяющая наличие среди нормированных токенов слов из стоп-листа\n",
    "### и возвращающая их число, их порядковые номера, их форму из изначального текста\n",
    "def check_unigrams(tokens, tokens_normed, begin, end, blacklist):\n",
    "    bad_indexes = []\n",
    "    bad_words = []\n",
    "    score = 0\n",
    "    for i in range(begin, end):\n",
    "        if tokens_normed[i] in blacklist:\n",
    "            score+=1\n",
    "            bad_indexes.append(i)\n",
    "            bad_words.append(tokens[i])\n",
    "    return {'score':score, 'bad_indexes':bad_indexes, 'bad_words':bad_words}\n",
    "            \n",
    "def check_bigrams(tokens, tokens_normed, begin, end, blacklist):\n",
    "    bad_indexes = []\n",
    "    bad_words = []\n",
    "    score = 0\n",
    "    for i in range(begin, end-1):\n",
    "        if tokens_normed[i]+' '+tokens_normed[i+1] in blacklist:\n",
    "            score+=1\n",
    "            bad_indexes.append(i)\n",
    "            bad_indexes.append(i+1)\n",
    "            bad_words.append(tokens[i])\n",
    "            bad_words.append(tokens[i+1])\n",
    "    return {'score':score, 'bad_indexes':bad_indexes, 'bad_words':bad_words}\n",
    "\n",
    "\n",
    "### Главная функция, принимающая на вход json-файл из совы, бьщая его на блоки для анализа\n",
    "### Проверяющая эти блоки на слова из стоп-списка\n",
    "def check_text(json_file, drug_threshold = 2, lgbt_threshold = 1, suicide_threshold = 1, window_of_analysis = 100):\n",
    "    drug_words, suicide_words, suicide_bigrams, lgbt_words = get_stop_lists()    \n",
    "    tokens = nltk.word_tokenize(json_file['r'][0]['response'][0]['text'].lower())\n",
    "    tokens_normed = []\n",
    "    for token in tokens:\n",
    "        tokens_normed.append(morph.parse(token)[0].normal_form)\n",
    "    for low_border in range(0, len(tokens), window_of_analysis//2):\n",
    "        up_border = min(low_border+window_of_analysis, len(tokens)-1)\n",
    "        drug_report = check_unigrams(tokens, tokens_normed, low_border, up_border, drug_words)\n",
    "        lgbt_report = check_unigrams(tokens, tokens_normed, low_border, up_border, lgbt_words)\n",
    "        suicide_report = check_unigrams(tokens, tokens_normed, low_border, up_border, suicide_words)\n",
    "        suicide_report2 = check_bigrams(tokens, tokens_normed, low_border, up_border, suicide_bigrams)\n",
    "        if drug_report['score'] > drug_threshold:\n",
    "            print(drug_report['bad_words'])\n",
    "            #pass Здесь логика передающая репорт с указанием  begin, end, bad_words \n",
    "        if lgbt_report['score'] > lgbt_threshold:\n",
    "            print(lgbt_report['bad_words'])\n",
    "        if suicide_report['score'] + suicide_report2['score'] > suicide_threshold:\n",
    "            print(suicide_report['bad_words'])\n",
    "            print(suicide_report2['bad_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f066429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['нарко', 'кокаин', 'кокс', 'декстра', 'метамфетамин', 'гашиш', 'гаш', 'метадон', 'героин', 'герыч', 'гидропоника', 'шишка', 'альфа', 'вес', 'дозировка']\n",
      "['героин', 'герыч', 'гидропоника', 'шишка', 'альфа', 'вес', 'дозировка']\n",
      "['веществами', 'мефедрон', 'наркотиками']\n",
      "['шишка', 'наркотики', 'мефедрона', 'наркотики']\n",
      "['наркотики', 'мефедрона', 'наркотики', 'шишка']\n",
      "['кокс', 'гашиш', 'нарко', 'экстази', 'трипа']\n",
      "['кокс', 'гашиш', 'нарко', 'экстази', 'трипа', 'наркоману']\n",
      "['наркотики', 'конопля', 'марихуана']\n",
      "['наркотиков', 'конопли', 'гашиша', 'наркотик', 'марки']\n",
      "['гашиша', 'наркотик', 'марки']\n",
      "['гашиш', 'нарко', 'героин', 'гашиш', 'кокаин']\n",
      "['нарко', 'героин', 'гашиш', 'кокаин', 'марихуана']\n",
      "['шишки', 'нарко', 'наркотики', 'наркотики']\n",
      "['наркотики', 'наркотики', 'кристаллы', 'траву', 'мефедрон']\n",
      "['кристаллы', 'траву', 'мефедрон', 'травы', 'героина']\n",
      "['нарко', 'вещество', 'героина']\n",
      "['наркотика', 'шишки', 'трип', 'трава', 'гашиш']\n",
      "['шишки', 'трип', 'трава', 'гашиш']\n",
      "['нарко', 'наркотики', 'дозировки']\n",
      "['однополые', 'однополыми']\n",
      "['однополых', 'однополых']\n",
      "['феминизма', 'аутинг', 'бисексуальность', 'дисфория', 'гомосексуальность', 'дисфории', 'диморфизм', 'демисексуалы', 'демисексуалы', 'феминизм', 'небинарный', 'асексуалы', 'чайлдфри']\n",
      "['небинарный', 'асексуалы', 'чайлдфри']\n",
      "['однополые', 'лгбт', 'однополые']\n",
      "['трансфобны', 'лгбт', 'небинарные']\n",
      "['трансфобны', 'лгбт', 'небинарные']\n",
      "['однополые', 'однополые', 'цисгендерные', 'лгбт']\n",
      "['однополые', 'цисгендерные', 'лгбт']\n",
      "['самоубийство']\n",
      "['убить', 'себя']\n",
      "[]\n",
      "['себя', 'убить', 'убить', 'себя']\n",
      "[]\n",
      "['себя', 'убить', 'убить', 'себя']\n",
      "['самоубийца']\n",
      "['убить', 'себя']\n",
      "[]\n",
      "['убить', 'себя', 'убить', 'себя']\n",
      "[]\n",
      "['убить', 'себя', 'убить', 'себя']\n",
      "[]\n",
      "['убьёшь', 'себя', 'убив', 'себя', 'убив', 'себя']\n",
      "['самоубийцу']\n",
      "['убьёшь', 'себя', 'убив', 'себя', 'убив', 'себя', 'убить', 'себя', 'убьёшь', 'себя']\n",
      "['самоубийцу']\n",
      "['убить', 'себя', 'убьёшь', 'себя', 'убьёшь', 'себя']\n",
      "['самоубийство', 'утопиться']\n",
      "[]\n",
      "['самоубийство', 'самоубийства']\n",
      "[]\n",
      "['самоубийство']\n",
      "['убить', 'себя', 'убить', 'себя']\n",
      "['самоубийство']\n",
      "['убить', 'себя']\n"
     ]
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "path_sova_new = os.path.join(cur_dir, 'texts_sova_2')\n",
    "filenames = []\n",
    "with os.scandir(path_sova_new) as files:\n",
    "    for file in files:\n",
    "        filenames.append(file)\n",
    "for file in filenames:\n",
    "    if str(file)[-6:-2] == '.txt':\n",
    "        with open (file, 'r', encoding = 'utf-8') as mfile:\n",
    "            x = mfile.read()\n",
    "            js = json.loads(x)\n",
    "            check_text(js)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
